"""
Meta-Reasoner LLM - Specialized reasoning about reasoning.

This module implements a specialized LLM for meta-reasoning about the system's
own reasoning processes. It analyzes patterns in reasoning failures, identifies
strategy improvements, and generates actionable insights for self-improvement.

Part of Phase 6: Heterogeneous Neural Ensemble (Issue #191).

Key capabilities:
- Strategy evaluation: which reasoning approaches work when
- Failure pattern analysis and root cause diagnosis
- Prompt optimization suggestions based on patterns
- Self-improvement goal generation

Architecture follows established ensemble patterns:
- Strategy Pattern for flexible meta-reasoning approaches
- Abstract base class for extensibility
- Exponential backoff retry logic for resilience
- LRU caching for repeated analyses
"""

from __future__ import annotations

import hashlib
import json
import random
import re
import threading
import time
from abc import ABC, abstractmethod
from concurrent.futures import ThreadPoolExecutor, TimeoutError as FuturesTimeoutError
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
from typing import Any, Dict, List, Optional, Type

from loguru import logger

from loft.neural.llm_interface import LLMInterface


# =============================================================================
# Custom Exceptions
# =============================================================================


class MetaReasoningError(Exception):
    """Custom exception for meta-reasoning failures after all retries."""

    def __init__(self, message: str, attempts: int = 0, last_error: Optional[str] = None):
        super().__init__(message)
        self.attempts = attempts
        self.last_error = last_error


# =============================================================================
# Enums
# =============================================================================


class MetaReasoningStrategyType(Enum):
    """Enum identifying meta-reasoning strategy types."""

    ANALYTICAL = "analytical"  # Deep analysis of patterns and root causes
    REFLECTIVE = "reflective"  # Second-order reasoning about reasoning
    COMPARATIVE = "comparative"  # Compare strategies across contexts
    DIAGNOSTIC = "diagnostic"  # Focus on failure diagnosis


class InsightType(Enum):
    """Types of insights generated by meta-reasoning."""

    PATTERN_IDENTIFIED = "pattern_identified"
    ROOT_CAUSE = "root_cause"
    STRATEGY_RECOMMENDATION = "strategy_recommendation"
    PROMPT_IMPROVEMENT = "prompt_improvement"
    CONFIDENCE_CALIBRATION = "confidence_calibration"
    KNOWLEDGE_GAP = "knowledge_gap"


class FailureCategory(Enum):
    """Categories of reasoning failures."""

    SYNTAX_ERROR = "syntax_error"
    SEMANTIC_ERROR = "semantic_error"
    GROUNDING_ERROR = "grounding_error"
    VALIDATION_ERROR = "validation_error"
    TIMEOUT_ERROR = "timeout_error"
    CONFIDENCE_ERROR = "confidence_error"
    UNKNOWN = "unknown"


# =============================================================================
# Data Classes
# =============================================================================


@dataclass
class FailureRecord:
    """Record of a reasoning failure for analysis.

    Attributes:
        failure_id: Unique identifier for this failure
        timestamp: When the failure occurred
        category: Failure category
        error_message: Error message if available
        context: Contextual information about the failure
        domain: Legal domain involved
        strategy_used: Strategy that was used
        input_summary: Summary of input that caused failure
        confidence_before: Confidence before failure
    """

    failure_id: str
    timestamp: datetime = field(default_factory=datetime.now)
    category: FailureCategory = FailureCategory.UNKNOWN
    error_message: Optional[str] = None
    context: Dict[str, Any] = field(default_factory=dict)
    domain: str = "unknown"
    strategy_used: str = "unknown"
    input_summary: str = ""
    confidence_before: float = 0.0


@dataclass
class Insight:
    """An actionable insight generated by meta-reasoning.

    Attributes:
        insight_id: Unique identifier
        insight_type: Type of insight
        title: Short title describing the insight
        description: Detailed description
        evidence: Supporting evidence
        confidence: Confidence in this insight (0-1)
        actionable: Whether this insight is actionable
        recommended_action: Specific action to take
        priority: Priority level (high, medium, low)
        related_failures: IDs of related failures
    """

    insight_id: str
    insight_type: InsightType
    title: str
    description: str
    evidence: List[str] = field(default_factory=list)
    confidence: float = 0.0
    actionable: bool = True
    recommended_action: Optional[str] = None
    priority: str = "medium"
    related_failures: List[str] = field(default_factory=list)

    def to_dict(self) -> Dict[str, Any]:
        """Serialize to dictionary."""
        return {
            "insight_id": self.insight_id,
            "insight_type": self.insight_type.value,
            "title": self.title,
            "description": self.description,
            "evidence": self.evidence,
            "confidence": self.confidence,
            "actionable": self.actionable,
            "recommended_action": self.recommended_action,
            "priority": self.priority,
            "related_failures": self.related_failures,
        }


@dataclass
class StrategyChange:
    """A recommended change to reasoning strategy.

    Attributes:
        change_id: Unique identifier
        current_strategy: Current strategy being used
        recommended_strategy: Recommended strategy
        context_conditions: When this change should apply
        expected_improvement: Expected improvement description
        confidence: Confidence in this recommendation
        rationale: Reasoning behind the recommendation
    """

    change_id: str
    current_strategy: str
    recommended_strategy: str
    context_conditions: Dict[str, Any] = field(default_factory=dict)
    expected_improvement: str = ""
    confidence: float = 0.0
    rationale: str = ""

    def to_dict(self) -> Dict[str, Any]:
        """Serialize to dictionary."""
        return {
            "change_id": self.change_id,
            "current_strategy": self.current_strategy,
            "recommended_strategy": self.recommended_strategy,
            "context_conditions": self.context_conditions,
            "expected_improvement": self.expected_improvement,
            "confidence": self.confidence,
            "rationale": self.rationale,
        }


@dataclass
class ImprovementReport:
    """Report assessing system's self-improvement trajectory.

    Attributes:
        report_id: Unique identifier
        timestamp: When report was generated
        overall_trajectory: Assessment of improvement trajectory
        metrics_summary: Summary of key metrics
        insights: List of insights
        strategy_changes: Recommended strategy changes
        prompt_improvements: Suggested prompt improvements
        confidence_calibration: Confidence calibration assessment
        next_goals: Recommended next improvement goals
    """

    report_id: str
    timestamp: datetime = field(default_factory=datetime.now)
    overall_trajectory: str = "stable"  # improving, stable, declining
    metrics_summary: Dict[str, float] = field(default_factory=dict)
    insights: List[Insight] = field(default_factory=list)
    strategy_changes: List[StrategyChange] = field(default_factory=list)
    prompt_improvements: List[str] = field(default_factory=list)
    confidence_calibration: Dict[str, Any] = field(default_factory=dict)
    next_goals: List[str] = field(default_factory=list)

    def to_dict(self) -> Dict[str, Any]:
        """Serialize to dictionary."""
        return {
            "report_id": self.report_id,
            "timestamp": self.timestamp.isoformat(),
            "overall_trajectory": self.overall_trajectory,
            "metrics_summary": self.metrics_summary,
            "insights": [i.to_dict() for i in self.insights],
            "strategy_changes": [s.to_dict() for s in self.strategy_changes],
            "prompt_improvements": self.prompt_improvements,
            "confidence_calibration": self.confidence_calibration,
            "next_goals": self.next_goals,
        }


@dataclass
class MetaReasoningResult:
    """Result from a meta-reasoning analysis.

    Attributes:
        result_id: Unique identifier
        timestamp: When analysis was performed
        analysis_type: Type of analysis performed
        insights: Generated insights
        confidence: Overall confidence in results
        reasoning_trace: Trace of reasoning process
        processing_time_ms: Processing time in milliseconds
    """

    result_id: str
    timestamp: datetime = field(default_factory=datetime.now)
    analysis_type: str = ""
    insights: List[Insight] = field(default_factory=list)
    confidence: float = 0.0
    reasoning_trace: List[str] = field(default_factory=list)
    processing_time_ms: float = 0.0

    def to_dict(self) -> Dict[str, Any]:
        """Serialize to dictionary."""
        return {
            "result_id": self.result_id,
            "timestamp": self.timestamp.isoformat(),
            "analysis_type": self.analysis_type,
            "insights": [i.to_dict() for i in self.insights],
            "confidence": self.confidence,
            "reasoning_trace": self.reasoning_trace,
            "processing_time_ms": self.processing_time_ms,
        }


# =============================================================================
# Configuration
# =============================================================================


@dataclass
class MetaReasonerConfig:
    """Configuration for MetaReasonerLLM.

    Attributes:
        model: LLM model identifier (for reference/logging)
        temperature: Temperature for generation
        strategy: Meta-reasoning strategy type
        max_retries: Maximum retry attempts
        retry_base_delay_seconds: Base delay for exponential backoff
        retry_jitter_max_seconds: Maximum jitter added to retry delays
        timeout_seconds: Timeout for individual LLM calls (None = no timeout)
        enable_cache: Enable caching
        cache_max_size: Maximum cache entries
        max_input_length: Maximum allowed input length
        enable_input_sanitization: Enable basic input sanitization
        min_confidence_threshold: Minimum confidence for insights
    """

    model: str = "claude-3-5-haiku-20241022"
    temperature: float = 0.3
    strategy: MetaReasoningStrategyType = MetaReasoningStrategyType.ANALYTICAL
    max_retries: int = 3
    retry_base_delay_seconds: float = 1.0
    retry_jitter_max_seconds: float = 0.5
    timeout_seconds: Optional[float] = 60.0
    enable_cache: bool = True
    cache_max_size: int = 100
    max_input_length: int = 15000
    enable_input_sanitization: bool = True
    min_confidence_threshold: float = 0.5

    def __post_init__(self) -> None:
        """Validate configuration values after initialization."""
        if not 0.0 <= self.temperature <= 2.0:
            raise ValueError(f"temperature must be between 0.0 and 2.0, got {self.temperature}")
        if self.max_retries < 0:
            raise ValueError(f"max_retries must be >= 0, got {self.max_retries}")
        if self.retry_base_delay_seconds < 0:
            raise ValueError(
                f"retry_base_delay_seconds must be >= 0, got {self.retry_base_delay_seconds}"
            )
        if self.retry_jitter_max_seconds < 0:
            raise ValueError(
                f"retry_jitter_max_seconds must be >= 0, got {self.retry_jitter_max_seconds}"
            )
        if self.timeout_seconds is not None and self.timeout_seconds <= 0:
            raise ValueError(f"timeout_seconds must be > 0 or None, got {self.timeout_seconds}")
        if self.cache_max_size < 0:
            raise ValueError(f"cache_max_size must be >= 0, got {self.cache_max_size}")
        if self.max_input_length <= 0:
            raise ValueError(f"max_input_length must be > 0, got {self.max_input_length}")
        if not 0.0 <= self.min_confidence_threshold <= 1.0:
            raise ValueError(
                f"min_confidence_threshold must be between 0.0 and 1.0, got {self.min_confidence_threshold}"
            )


# =============================================================================
# Strategy Pattern: Abstract base and concrete strategies
# =============================================================================


class MetaReasoningStrategy(ABC):
    """Abstract base class for meta-reasoning strategies.

    Follows the Strategy pattern to allow different meta-reasoning
    approaches to be swapped without changing the MetaReasonerLLM class.
    """

    @property
    @abstractmethod
    def strategy_type(self) -> MetaReasoningStrategyType:
        """Return the strategy type enum."""
        pass

    @abstractmethod
    def prepare_failure_analysis_prompt(
        self,
        failures: List[FailureRecord],
        context: Optional[Dict[str, Any]] = None,
    ) -> str:
        """Prepare prompt for failure pattern analysis.

        Args:
            failures: List of failure records to analyze
            context: Additional context

        Returns:
            Prepared prompt string
        """
        pass

    @abstractmethod
    def prepare_strategy_evaluation_prompt(
        self,
        performance_data: Dict[str, Any],
        context: Optional[Dict[str, Any]] = None,
    ) -> str:
        """Prepare prompt for strategy evaluation.

        Args:
            performance_data: Performance metrics by strategy
            context: Additional context

        Returns:
            Prepared prompt string
        """
        pass

    @abstractmethod
    def prepare_prompt_optimization_prompt(
        self,
        prompt: str,
        failures: List[FailureRecord],
        context: Optional[Dict[str, Any]] = None,
    ) -> str:
        """Prepare prompt for prompt optimization suggestions.

        Args:
            prompt: Current prompt to optimize
            failures: Related failures
            context: Additional context

        Returns:
            Prepared prompt string
        """
        pass

    @abstractmethod
    def prepare_self_assessment_prompt(
        self,
        metrics: Dict[str, Any],
        context: Optional[Dict[str, Any]] = None,
    ) -> str:
        """Prepare prompt for self-improvement assessment.

        Args:
            metrics: System metrics
            context: Additional context

        Returns:
            Prepared prompt string
        """
        pass


class AnalyticalStrategy(MetaReasoningStrategy):
    """Analytical strategy for deep pattern analysis."""

    @property
    def strategy_type(self) -> MetaReasoningStrategyType:
        return MetaReasoningStrategyType.ANALYTICAL

    def prepare_failure_analysis_prompt(
        self,
        failures: List[FailureRecord],
        context: Optional[Dict[str, Any]] = None,
    ) -> str:
        failure_summaries = []
        for f in failures[:20]:  # Limit to avoid token overflow
            failure_summaries.append(
                f"- [{f.category.value}] {f.error_message or 'No message'} "
                f"(domain: {f.domain}, strategy: {f.strategy_used})"
            )

        failures_text = "\n".join(failure_summaries) if failure_summaries else "None"
        context_text = json.dumps(context, indent=2) if context else "{}"

        return f"""You are an expert meta-reasoning system analyzing patterns in reasoning failures.

## Failures to Analyze
{failures_text}

## Context
{context_text}

## Task
Perform a deep analytical examination of these failures:

1. **Pattern Identification**: Identify recurring patterns across failures
2. **Root Cause Analysis**: Determine underlying causes, not just symptoms
3. **Category Analysis**: Which failure categories are most common and why?
4. **Domain Correlation**: Are failures correlated with specific domains?
5. **Strategy Effectiveness**: Which strategies led to failures?

## Response Format (JSON)
{{
    "patterns": [
        {{
            "pattern_id": "P1",
            "description": "Description of pattern",
            "frequency": 0.0,
            "severity": "high|medium|low",
            "examples": ["example1", "example2"]
        }}
    ],
    "root_causes": [
        {{
            "cause_id": "RC1",
            "description": "Root cause description",
            "affected_patterns": ["P1"],
            "confidence": 0.0
        }}
    ],
    "recommendations": [
        {{
            "recommendation_id": "R1",
            "title": "Short title",
            "description": "Detailed recommendation",
            "priority": "high|medium|low",
            "expected_impact": "Description of expected impact"
        }}
    ],
    "confidence": 0.0,
    "reasoning_summary": "Brief summary of reasoning process"
}}

Respond only with valid JSON."""

    def prepare_strategy_evaluation_prompt(
        self,
        performance_data: Dict[str, Any],
        context: Optional[Dict[str, Any]] = None,
    ) -> str:
        perf_text = json.dumps(performance_data, indent=2)
        context_text = json.dumps(context, indent=2) if context else "{}"

        return f"""You are an expert meta-reasoning system evaluating reasoning strategies.

## Performance Data by Strategy
{perf_text}

## Context
{context_text}

## Task
Analytically evaluate which reasoning strategies work best in different contexts:

1. **Performance Analysis**: Which strategies perform best overall?
2. **Context Sensitivity**: How does performance vary by domain/context?
3. **Failure Modes**: What are the characteristic failures of each strategy?
4. **Optimal Selection**: When should each strategy be selected?
5. **Improvement Opportunities**: How could underperforming strategies be improved?

## Response Format (JSON)
{{
    "strategy_rankings": [
        {{
            "strategy": "strategy_name",
            "overall_score": 0.0,
            "best_contexts": ["context1"],
            "worst_contexts": ["context2"],
            "key_strengths": ["strength1"],
            "key_weaknesses": ["weakness1"]
        }}
    ],
    "recommendations": [
        {{
            "current_strategy": "strategy_a",
            "recommended_strategy": "strategy_b",
            "conditions": {{"domain": "contracts"}},
            "expected_improvement": "Description",
            "confidence": 0.0,
            "rationale": "Why this change"
        }}
    ],
    "overall_assessment": "Summary assessment",
    "confidence": 0.0
}}

Respond only with valid JSON."""

    def prepare_prompt_optimization_prompt(
        self,
        prompt: str,
        failures: List[FailureRecord],
        context: Optional[Dict[str, Any]] = None,
    ) -> str:
        # Truncate prompt if too long
        prompt_preview = prompt[:2000] + "..." if len(prompt) > 2000 else prompt

        failure_summaries = []
        for f in failures[:10]:
            failure_summaries.append(
                f"- {f.error_message or 'No message'} (category: {f.category.value})"
            )
        failures_text = "\n".join(failure_summaries) if failure_summaries else "None"

        return f"""You are an expert at optimizing prompts for reasoning systems.

## Current Prompt
```
{prompt_preview}
```

## Related Failures
{failures_text}

## Task
Analyze the prompt and suggest specific improvements:

1. **Clarity Issues**: Are instructions clear and unambiguous?
2. **Missing Guidance**: What guidance is missing that led to failures?
3. **Structure**: Is the prompt well-structured?
4. **Examples**: Would examples help?
5. **Constraints**: Are necessary constraints clearly stated?

## Response Format (JSON)
{{
    "issues_identified": [
        {{
            "issue_id": "I1",
            "description": "Description of issue",
            "severity": "high|medium|low",
            "location": "Where in prompt"
        }}
    ],
    "improvements": [
        {{
            "improvement_id": "IMP1",
            "description": "What to change",
            "rationale": "Why this helps",
            "before": "Original text (if applicable)",
            "after": "Improved text"
        }}
    ],
    "optimized_prompt_summary": "Summary of key changes",
    "expected_improvement": "Expected improvement in failure rate",
    "confidence": 0.0
}}

Respond only with valid JSON."""

    def prepare_self_assessment_prompt(
        self,
        metrics: Dict[str, Any],
        context: Optional[Dict[str, Any]] = None,
    ) -> str:
        metrics_text = json.dumps(metrics, indent=2)
        context_text = json.dumps(context, indent=2) if context else "{}"

        return f"""You are a meta-reasoning system assessing your own improvement trajectory.

## Current Metrics
{metrics_text}

## Context
{context_text}

## Task
Perform a comprehensive self-assessment:

1. **Trajectory Analysis**: Is the system improving, stable, or declining?
2. **Strength Assessment**: What is the system doing well?
3. **Gap Analysis**: Where are the biggest gaps?
4. **Confidence Calibration**: Is confidence aligned with actual performance?
5. **Next Steps**: What should be the priority improvement goals?

Second-order reflection: Consider why your own reasoning about these metrics
might be biased or incomplete.

## Response Format (JSON)
{{
    "overall_trajectory": "improving|stable|declining",
    "trajectory_confidence": 0.0,
    "strengths": [
        {{
            "area": "Area of strength",
            "evidence": "Supporting evidence",
            "confidence": 0.0
        }}
    ],
    "gaps": [
        {{
            "area": "Area needing improvement",
            "severity": "high|medium|low",
            "evidence": "Supporting evidence"
        }}
    ],
    "confidence_calibration": {{
        "is_well_calibrated": true,
        "calibration_issues": ["issue1"],
        "recommendations": ["recommendation1"]
    }},
    "next_goals": [
        {{
            "goal": "Specific goal",
            "priority": "high|medium|low",
            "rationale": "Why this goal",
            "success_criteria": "How to measure success"
        }}
    ],
    "epistemic_humility_note": "Acknowledgment of limitations in this assessment",
    "overall_confidence": 0.0
}}

Respond only with valid JSON."""


class ReflectiveStrategy(MetaReasoningStrategy):
    """Reflective strategy for second-order reasoning about reasoning."""

    @property
    def strategy_type(self) -> MetaReasoningStrategyType:
        return MetaReasoningStrategyType.REFLECTIVE

    def prepare_failure_analysis_prompt(
        self,
        failures: List[FailureRecord],
        context: Optional[Dict[str, Any]] = None,
    ) -> str:
        failure_summaries = []
        for f in failures[:15]:
            failure_summaries.append(f"- [{f.category.value}] {f.error_message or 'No message'}")
        failures_text = "\n".join(failure_summaries) if failure_summaries else "None"

        return f"""You are a reflective meta-reasoning system. Your task is to reason
about your own reasoning processes when analyzing failures.

## Failures
{failures_text}

## Reflective Analysis Task
Engage in second-order reasoning:

1. "My reasoning about failure X was flawed because..."
2. "The pattern I identified might be wrong because..."
3. "I am uncertain about Y because..."
4. "Alternative explanations I might be missing include..."

Consider cognitive biases that might affect your analysis:
- Confirmation bias: Am I seeing patterns I expect?
- Availability heuristic: Am I overweighting recent/salient failures?
- Anchoring: Am I fixed on initial interpretations?

## Response Format (JSON)
{{
    "first_order_analysis": {{
        "patterns": ["pattern1"],
        "root_causes": ["cause1"]
    }},
    "second_order_reflection": {{
        "reasoning_flaws": ["My analysis might be flawed because..."],
        "alternative_explanations": ["Alternative explanation"],
        "uncertainty_acknowledgments": ["I am uncertain about..."],
        "bias_checks": ["I checked for X bias and found..."]
    }},
    "refined_insights": [
        {{
            "insight": "Refined insight after reflection",
            "confidence": 0.0,
            "confidence_justification": "Why this confidence level"
        }}
    ],
    "epistemic_status": "Description of knowledge state",
    "overall_confidence": 0.0
}}

Respond only with valid JSON."""

    def prepare_strategy_evaluation_prompt(
        self,
        performance_data: Dict[str, Any],
        context: Optional[Dict[str, Any]] = None,
    ) -> str:
        perf_text = json.dumps(performance_data, indent=2)

        return f"""You are a reflective meta-reasoning system evaluating strategies
through second-order reasoning.

## Performance Data
{perf_text}

## Reflective Evaluation Task
Think about your thinking process as you evaluate strategies:

1. First, what does the data suggest about strategy effectiveness?
2. Then, what assumptions am I making in this interpretation?
3. What would change my conclusions?
4. Where might my evaluation be biased or incomplete?

## Response Format (JSON)
{{
    "initial_evaluation": {{
        "best_strategy": "strategy_name",
        "reasoning": "Why I think this"
    }},
    "assumption_check": {{
        "assumptions_made": ["assumption1"],
        "validity_of_assumptions": ["assessment1"]
    }},
    "sensitivity_analysis": {{
        "what_would_change_conclusion": ["factor1"],
        "confidence_bounds": {{"lower": 0.0, "upper": 0.0}}
    }},
    "refined_recommendations": [
        {{
            "recommendation": "recommendation",
            "confidence": 0.0,
            "epistemic_status": "certain|probable|uncertain|speculative"
        }}
    ],
    "overall_confidence": 0.0
}}

Respond only with valid JSON."""

    def prepare_prompt_optimization_prompt(
        self,
        prompt: str,
        failures: List[FailureRecord],
        context: Optional[Dict[str, Any]] = None,
    ) -> str:
        prompt_preview = prompt[:1500] + "..." if len(prompt) > 1500 else prompt

        return f"""You are a reflective meta-reasoning system optimizing prompts.

## Current Prompt
```
{prompt_preview}
```

## Reflective Optimization Task
Reason about your reasoning as you analyze this prompt:

1. What issues do I see? Why do I think they are issues?
2. Am I projecting my own interpretation onto the prompt?
3. How might different systems interpret this prompt differently?
4. What implicit assumptions does this prompt make?

## Response Format (JSON)
{{
    "surface_analysis": {{
        "obvious_issues": ["issue1"],
        "initial_improvements": ["improvement1"]
    }},
    "deeper_reflection": {{
        "implicit_assumptions": ["assumption1"],
        "interpretation_variations": ["different interpretation"],
        "my_biases": ["bias I might have"]
    }},
    "refined_improvements": [
        {{
            "improvement": "specific improvement",
            "confidence": 0.0,
            "why_this_helps": "reasoning"
        }}
    ],
    "overall_confidence": 0.0
}}

Respond only with valid JSON."""

    def prepare_self_assessment_prompt(
        self,
        metrics: Dict[str, Any],
        context: Optional[Dict[str, Any]] = None,
    ) -> str:
        metrics_text = json.dumps(metrics, indent=2)

        return f"""You are a reflective meta-reasoning system assessing yourself.

## Metrics
{metrics_text}

## Reflective Self-Assessment
This is second-order reflection: reasoning about your reasoning about yourself.

1. What do these metrics tell me about system performance?
2. What do they NOT tell me? What's missing?
3. How might I be misinterpreting these metrics?
4. What would a critic say about my self-assessment?
5. What am I blind to about my own performance?

## Response Format (JSON)
{{
    "initial_assessment": {{
        "trajectory": "improving|stable|declining",
        "key_observations": ["observation1"]
    }},
    "meta_reflection": {{
        "what_metrics_miss": ["missing aspect"],
        "potential_misinterpretations": ["misinterpretation"],
        "critic_perspective": "What a critic would say",
        "blind_spots": ["potential blind spot"]
    }},
    "calibrated_assessment": {{
        "trajectory": "improving|stable|declining",
        "confidence": 0.0,
        "caveats": ["caveat1"]
    }},
    "humble_next_steps": [
        {{
            "goal": "goal",
            "uncertainty": "what I'm unsure about",
            "fallback_if_wrong": "what to do if this is wrong"
        }}
    ],
    "overall_confidence": 0.0
}}

Respond only with valid JSON."""


class ComparativeStrategy(MetaReasoningStrategy):
    """Comparative strategy for cross-context analysis."""

    @property
    def strategy_type(self) -> MetaReasoningStrategyType:
        return MetaReasoningStrategyType.COMPARATIVE

    def prepare_failure_analysis_prompt(
        self,
        failures: List[FailureRecord],
        context: Optional[Dict[str, Any]] = None,
    ) -> str:
        # Group failures by domain
        by_domain: Dict[str, List[FailureRecord]] = {}
        for f in failures:
            by_domain.setdefault(f.domain, []).append(f)

        domain_summaries = []
        for domain, domain_failures in by_domain.items():
            categories = {}
            for f in domain_failures:
                categories[f.category.value] = categories.get(f.category.value, 0) + 1
            domain_summaries.append(
                f"- {domain}: {len(domain_failures)} failures, categories: {categories}"
            )

        summaries_text = "\n".join(domain_summaries) if domain_summaries else "None"

        return f"""You are a comparative meta-reasoning system analyzing failures
across different contexts.

## Failures by Domain
{summaries_text}

## Comparative Analysis Task
Compare failure patterns across domains and contexts:

1. **Cross-Domain Comparison**: How do failure patterns differ by domain?
2. **Common vs Unique**: What failures are common across domains vs unique?
3. **Transfer Learning**: What lessons from one domain apply to others?
4. **Differential Diagnosis**: Why does domain X fail differently than domain Y?

## Response Format (JSON)
{{
    "domain_profiles": [
        {{
            "domain": "domain_name",
            "failure_signature": ["characteristic failure types"],
            "unique_challenges": ["challenge specific to this domain"]
        }}
    ],
    "cross_domain_patterns": [
        {{
            "pattern": "pattern description",
            "domains_affected": ["domain1", "domain2"],
            "root_cause": "common root cause"
        }}
    ],
    "transfer_opportunities": [
        {{
            "from_domain": "domain_a",
            "to_domain": "domain_b",
            "lesson": "what can be transferred",
            "confidence": 0.0
        }}
    ],
    "recommendations_by_domain": {{
        "domain_name": ["recommendation1"]
    }},
    "overall_confidence": 0.0
}}

Respond only with valid JSON."""

    def prepare_strategy_evaluation_prompt(
        self,
        performance_data: Dict[str, Any],
        context: Optional[Dict[str, Any]] = None,
    ) -> str:
        perf_text = json.dumps(performance_data, indent=2)

        return f"""You are a comparative meta-reasoning system evaluating strategies.

## Performance Data
{perf_text}

## Comparative Evaluation Task
Compare strategies across different dimensions:

1. **Head-to-Head**: Direct comparison of strategy pairs
2. **Context Sensitivity**: Which strategies are more/less context-dependent?
3. **Trade-offs**: What are the trade-offs between strategies?
4. **Complementarity**: Which strategies complement each other?

## Response Format (JSON)
{{
    "pairwise_comparisons": [
        {{
            "strategy_a": "name",
            "strategy_b": "name",
            "winner_overall": "name or tie",
            "winner_by_context": {{"context1": "winner"}}
        }}
    ],
    "context_sensitivity_ranking": [
        {{
            "strategy": "name",
            "sensitivity_score": 0.0,
            "most_sensitive_to": ["context factor"]
        }}
    ],
    "trade_off_analysis": [
        {{
            "trade_off": "description",
            "strategies_involved": ["strategy1", "strategy2"]
        }}
    ],
    "complementary_pairs": [
        {{
            "strategies": ["strategy1", "strategy2"],
            "synergy_description": "how they complement"
        }}
    ],
    "overall_confidence": 0.0
}}

Respond only with valid JSON."""

    def prepare_prompt_optimization_prompt(
        self,
        prompt: str,
        failures: List[FailureRecord],
        context: Optional[Dict[str, Any]] = None,
    ) -> str:
        prompt_preview = prompt[:1500] + "..." if len(prompt) > 1500 else prompt

        # Compare to common prompt patterns
        return f"""You are a comparative meta-reasoning system optimizing prompts.

## Current Prompt
```
{prompt_preview}
```

## Comparative Optimization Task
Compare this prompt to best practices and alternative approaches:

1. **Best Practice Comparison**: How does this compare to prompt best practices?
2. **Alternative Formulations**: What are alternative ways to prompt for this?
3. **Component Analysis**: Which parts are strong/weak compared to alternatives?

## Response Format (JSON)
{{
    "best_practice_gaps": [
        {{
            "best_practice": "description",
            "current_gap": "how this prompt falls short",
            "improvement": "suggested improvement"
        }}
    ],
    "alternative_approaches": [
        {{
            "approach": "alternative approach",
            "pros": ["pro1"],
            "cons": ["con1"],
            "recommendation": "use|consider|avoid"
        }}
    ],
    "component_comparison": {{
        "strong_components": ["component1"],
        "weak_components": ["component2"],
        "missing_components": ["component3"]
    }},
    "overall_confidence": 0.0
}}

Respond only with valid JSON."""

    def prepare_self_assessment_prompt(
        self,
        metrics: Dict[str, Any],
        context: Optional[Dict[str, Any]] = None,
    ) -> str:
        metrics_text = json.dumps(metrics, indent=2)

        return f"""You are a comparative meta-reasoning system assessing performance.

## Current Metrics
{metrics_text}

## Comparative Self-Assessment
Compare current performance to baselines and alternatives:

1. **Historical Comparison**: How does current performance compare to past?
2. **Benchmark Comparison**: How does it compare to reasonable benchmarks?
3. **Component Comparison**: Which components are strong/weak relatively?
4. **Opportunity Cost**: What would alternative approaches achieve?

## Response Format (JSON)
{{
    "trend_analysis": {{
        "direction": "improving|stable|declining",
        "rate_of_change": "fast|moderate|slow",
        "consistency": "consistent|variable"
    }},
    "benchmark_comparison": [
        {{
            "benchmark": "benchmark name",
            "current_vs_benchmark": "above|at|below",
            "gap_size": "large|moderate|small|none"
        }}
    ],
    "relative_strengths": [
        {{
            "component": "component name",
            "relative_rank": "strong|average|weak",
            "comparison_basis": "what compared to"
        }}
    ],
    "opportunity_analysis": {{
        "current_approach_score": 0.0,
        "alternative_potential": 0.0,
        "recommendation": "stay course|minor adjust|major change"
    }},
    "overall_confidence": 0.0
}}

Respond only with valid JSON."""


class DiagnosticStrategy(MetaReasoningStrategy):
    """Diagnostic strategy focused on failure diagnosis."""

    @property
    def strategy_type(self) -> MetaReasoningStrategyType:
        return MetaReasoningStrategyType.DIAGNOSTIC

    def prepare_failure_analysis_prompt(
        self,
        failures: List[FailureRecord],
        context: Optional[Dict[str, Any]] = None,
    ) -> str:
        failure_details = []
        for f in failures[:15]:
            failure_details.append(
                {
                    "id": f.failure_id,
                    "category": f.category.value,
                    "message": f.error_message,
                    "domain": f.domain,
                    "strategy": f.strategy_used,
                    "confidence_before": f.confidence_before,
                }
            )
        failures_json = json.dumps(failure_details, indent=2)

        return f"""You are a diagnostic meta-reasoning system specializing in
failure root cause analysis.

## Failure Data
{failures_json}

## Diagnostic Task
Perform systematic failure diagnosis:

1. **Symptom Analysis**: What are the observable symptoms?
2. **Differential Diagnosis**: What are possible causes for each symptom?
3. **Root Cause Identification**: What is the most likely root cause?
4. **Causal Chain**: What is the chain of causation?
5. **Treatment Plan**: What specific fixes address the root cause?

## Response Format (JSON)
{{
    "symptom_cluster": [
        {{
            "symptom": "observable symptom",
            "frequency": 0.0,
            "severity": "critical|major|minor"
        }}
    ],
    "differential_diagnosis": [
        {{
            "symptom": "symptom",
            "possible_causes": [
                {{
                    "cause": "possible cause",
                    "likelihood": 0.0,
                    "supporting_evidence": ["evidence"],
                    "contradicting_evidence": ["evidence"]
                }}
            ]
        }}
    ],
    "root_cause_analysis": [
        {{
            "root_cause": "identified root cause",
            "confidence": 0.0,
            "causal_chain": ["step1", "step2", "symptom"],
            "affected_failures": ["failure_id1"]
        }}
    ],
    "treatment_plan": [
        {{
            "intervention": "specific fix",
            "targets_root_cause": "which root cause",
            "priority": "immediate|short_term|long_term",
            "expected_effectiveness": 0.0
        }}
    ],
    "overall_confidence": 0.0
}}

Respond only with valid JSON."""

    def prepare_strategy_evaluation_prompt(
        self,
        performance_data: Dict[str, Any],
        context: Optional[Dict[str, Any]] = None,
    ) -> str:
        perf_text = json.dumps(performance_data, indent=2)

        return f"""You are a diagnostic meta-reasoning system evaluating strategies.

## Performance Data
{perf_text}

## Diagnostic Evaluation Task
Diagnose strategy performance issues:

1. **Performance Symptoms**: What symptoms indicate problems?
2. **Root Causes**: Why is each strategy performing as it is?
3. **Systemic Issues**: Are there systemic issues affecting multiple strategies?
4. **Prescriptions**: What specific changes would fix identified issues?

## Response Format (JSON)
{{
    "strategy_diagnoses": [
        {{
            "strategy": "strategy_name",
            "performance_symptoms": ["symptom1"],
            "diagnosed_causes": ["cause1"],
            "prescribed_fixes": ["fix1"]
        }}
    ],
    "systemic_issues": [
        {{
            "issue": "systemic issue description",
            "strategies_affected": ["strategy1"],
            "root_cause": "why this is systemic",
            "systemic_fix": "how to address"
        }}
    ],
    "priority_fixes": [
        {{
            "fix": "specific fix",
            "impact": "high|medium|low",
            "effort": "high|medium|low",
            "priority_score": 0.0
        }}
    ],
    "overall_confidence": 0.0
}}

Respond only with valid JSON."""

    def prepare_prompt_optimization_prompt(
        self,
        prompt: str,
        failures: List[FailureRecord],
        context: Optional[Dict[str, Any]] = None,
    ) -> str:
        prompt_preview = prompt[:1500] + "..." if len(prompt) > 1500 else prompt
        failure_messages = [f.error_message or "No message" for f in failures[:10]]

        return f"""You are a diagnostic meta-reasoning system optimizing prompts.

## Prompt to Diagnose
```
{prompt_preview}
```

## Associated Failure Messages
{json.dumps(failure_messages, indent=2)}

## Diagnostic Optimization Task
Diagnose why this prompt is causing failures:

1. **Failure-Prompt Correlation**: Which prompt elements correlate with failures?
2. **Ambiguity Detection**: Where is the prompt ambiguous?
3. **Missing Information**: What information is the prompt missing?
4. **Targeted Fixes**: What minimal changes would prevent the failures?

## Response Format (JSON)
{{
    "failure_correlations": [
        {{
            "prompt_element": "element causing issues",
            "associated_failures": ["failure type"],
            "correlation_strength": 0.0
        }}
    ],
    "ambiguity_issues": [
        {{
            "location": "where in prompt",
            "ambiguity": "what's ambiguous",
            "possible_interpretations": ["interpretation1", "interpretation2"]
        }}
    ],
    "information_gaps": [
        {{
            "gap": "what's missing",
            "impact": "how it causes failures",
            "suggested_addition": "what to add"
        }}
    ],
    "targeted_fixes": [
        {{
            "fix": "specific change",
            "prevents": "which failures",
            "confidence": 0.0
        }}
    ],
    "overall_confidence": 0.0
}}

Respond only with valid JSON."""

    def prepare_self_assessment_prompt(
        self,
        metrics: Dict[str, Any],
        context: Optional[Dict[str, Any]] = None,
    ) -> str:
        metrics_text = json.dumps(metrics, indent=2)

        return f"""You are a diagnostic meta-reasoning system assessing system health.

## System Metrics
{metrics_text}

## Diagnostic Self-Assessment
Diagnose the health of the system:

1. **Vital Signs**: What are the key health indicators?
2. **Anomalies**: What metrics are abnormal?
3. **Diagnosis**: What issues do the metrics reveal?
4. **Prognosis**: What is the likely trajectory without intervention?
5. **Treatment**: What interventions are needed?

## Response Format (JSON)
{{
    "vital_signs": [
        {{
            "metric": "metric name",
            "value": "current value",
            "status": "healthy|warning|critical",
            "normal_range": "expected range"
        }}
    ],
    "anomalies": [
        {{
            "metric": "metric name",
            "anomaly": "description of anomaly",
            "significance": "high|medium|low"
        }}
    ],
    "diagnosis": [
        {{
            "condition": "diagnosed condition",
            "severity": "severe|moderate|mild",
            "evidence": ["supporting evidence"]
        }}
    ],
    "prognosis": {{
        "without_intervention": "expected trajectory",
        "with_intervention": "expected trajectory with fixes",
        "critical_timeframe": "urgency"
    }},
    "treatment_plan": [
        {{
            "intervention": "specific action",
            "priority": "immediate|soon|planned",
            "expected_outcome": "what it should fix"
        }}
    ],
    "overall_confidence": 0.0
}}

Respond only with valid JSON."""


def create_meta_reasoning_strategy(
    strategy_type: MetaReasoningStrategyType,
) -> MetaReasoningStrategy:
    """Factory function to create a meta-reasoning strategy.

    Args:
        strategy_type: Type of strategy to create

    Returns:
        MetaReasoningStrategy instance
    """
    strategy_map: Dict[MetaReasoningStrategyType, Type[MetaReasoningStrategy]] = {
        MetaReasoningStrategyType.ANALYTICAL: AnalyticalStrategy,
        MetaReasoningStrategyType.REFLECTIVE: ReflectiveStrategy,
        MetaReasoningStrategyType.COMPARATIVE: ComparativeStrategy,
        MetaReasoningStrategyType.DIAGNOSTIC: DiagnosticStrategy,
    }

    strategy_class = strategy_map.get(strategy_type)
    if not strategy_class:
        raise ValueError(f"Unknown strategy type: {strategy_type}")

    return strategy_class()


# =============================================================================
# Abstract Base Class
# =============================================================================


class MetaReasoner(ABC):
    """Abstract base class for meta-reasoning components.

    Defines the interface for meta-reasoning about reasoning processes.
    """

    @abstractmethod
    def analyze_failure_patterns(self, failures: List[FailureRecord]) -> MetaReasoningResult:
        """Identify patterns in reasoning failures.

        Args:
            failures: List of failure records to analyze

        Returns:
            MetaReasoningResult with insights
        """
        pass

    @abstractmethod
    def suggest_strategy_changes(self, performance_data: Dict[str, Any]) -> List[StrategyChange]:
        """Recommend reasoning strategy modifications.

        Args:
            performance_data: Performance metrics by strategy

        Returns:
            List of recommended strategy changes
        """
        pass

    @abstractmethod
    def optimize_prompts(self, prompt: str, failures: List[FailureRecord]) -> List[str]:
        """Suggest prompt improvements based on failure analysis.

        Args:
            prompt: Current prompt to optimize
            failures: Related failures

        Returns:
            List of prompt improvement suggestions
        """
        pass

    @abstractmethod
    def assess_self_improvement(self, metrics: Dict[str, Any]) -> ImprovementReport:
        """Evaluate system's self-improvement trajectory.

        Args:
            metrics: System metrics

        Returns:
            ImprovementReport with assessment
        """
        pass


# =============================================================================
# Main Implementation
# =============================================================================


class MetaReasonerLLM(MetaReasoner):
    """LLM-based meta-reasoner for reasoning about reasoning.

    This class implements meta-level reasoning about the system's own
    reasoning processes, generating actionable insights for improvement.

    Features:
    - Strategy Pattern for flexible meta-reasoning approaches
    - Failure pattern analysis and root cause diagnosis
    - Strategy evaluation and recommendations
    - Prompt optimization suggestions
    - Self-improvement trajectory assessment
    - Caching for repeated analyses
    - Exponential backoff retry logic

    Example:
        >>> from loft.neural.llm_interface import LLMInterface
        >>> llm = LLMInterface(provider=my_provider)
        >>> config = MetaReasonerConfig(
        ...     strategy=MetaReasoningStrategyType.ANALYTICAL
        ... )
        >>> meta_reasoner = MetaReasonerLLM(llm, config)
        >>> failures = [FailureRecord(...), ...]
        >>> result = meta_reasoner.analyze_failure_patterns(failures)
        >>> print(result.insights)
    """

    def __init__(
        self,
        llm_interface: LLMInterface,
        config: Optional[MetaReasonerConfig] = None,
    ):
        """Initialize the MetaReasonerLLM.

        Args:
            llm_interface: Pre-configured LLM interface for making queries
            config: Configuration options
        """
        self.config = config or MetaReasonerConfig()

        # Initialize LLM interface (required)
        self._llm = llm_interface

        # Initialize strategy
        self._strategy = create_meta_reasoning_strategy(self.config.strategy)

        # Statistics tracking
        self._total_analyses = 0
        self._successful_analyses = 0
        self._failed_analyses = 0
        self._total_insights_generated = 0
        self._cache_hits = 0

        # Caching
        self._cache: Dict[str, MetaReasoningResult] = {}
        self._cache_lock = threading.Lock()

        logger.info(
            f"Initialized MetaReasonerLLM: "
            f"strategy={self._strategy.strategy_type.value}, "
            f"model={self.config.model}, "
            f"cache_enabled={self.config.enable_cache}"
        )

    def _validate_input(self, text: str, input_name: str = "input") -> None:
        """Validate input text length and basic constraints.

        Args:
            text: Input text to validate
            input_name: Name of the input parameter for error messages

        Raises:
            ValueError: If input exceeds maximum length
        """
        if len(text) > self.config.max_input_length:
            raise ValueError(
                f"{input_name} exceeds maximum length of "
                f"{self.config.max_input_length} characters "
                f"(got {len(text)} characters)"
            )

    def _sanitize_input(self, text: str) -> str:
        """Sanitize input text to remove potentially problematic patterns.

        Args:
            text: Input text to sanitize

        Returns:
            Sanitized text
        """
        if not self.config.enable_input_sanitization:
            return text

        sanitized = text

        # Remove control characters (except newlines and tabs)
        sanitized = re.sub(r"[\x00-\x08\x0b\x0c\x0e-\x1f\x7f]", "", sanitized)

        return sanitized

    def _calculate_retry_delay(self, attempt: int) -> float:
        """Calculate retry delay with exponential backoff and jitter.

        Args:
            attempt: Current attempt number (0-indexed)

        Returns:
            Delay in seconds
        """
        base_delay = self.config.retry_base_delay_seconds * (2**attempt)
        jitter = random.uniform(0, self.config.retry_jitter_max_seconds)
        return base_delay + jitter

    def _generate_cache_key(self, analysis_type: str, data: str) -> str:
        """Generate a cache key for analysis results.

        Args:
            analysis_type: Type of analysis
            data: Serialized input data

        Returns:
            Cache key string
        """
        content = f"{analysis_type}:{self._strategy.strategy_type.value}:{data}"
        return hashlib.sha256(content.encode()).hexdigest()[:16]

    def _get_cached_result(self, cache_key: str) -> Optional[MetaReasoningResult]:
        """Get cached result if available.

        Args:
            cache_key: Cache key to look up

        Returns:
            Cached result or None
        """
        if not self.config.enable_cache:
            return None

        with self._cache_lock:
            result = self._cache.get(cache_key)
            if result:
                self._cache_hits += 1
                logger.debug(f"Cache hit for key {cache_key}")
            return result

    def _cache_result(self, cache_key: str, result: MetaReasoningResult) -> None:
        """Cache an analysis result.

        Args:
            cache_key: Cache key
            result: Result to cache
        """
        if not self.config.enable_cache:
            return

        with self._cache_lock:
            # Implement simple LRU by removing oldest if at capacity
            if len(self._cache) >= self.config.cache_max_size:
                oldest_key = next(iter(self._cache))
                del self._cache[oldest_key]

            self._cache[cache_key] = result

    def _call_llm_with_retry(self, prompt: str, context: Optional[str] = None) -> str:
        """Call LLM with retry logic and timeout protection.

        Args:
            prompt: Prompt to send to LLM
            context: Optional context string for enhanced error logging

        Returns:
            LLM response string

        Raises:
            MetaReasoningError: If all retries fail or timeout occurs
        """
        last_error = None
        prompt_summary = prompt[:100] + "..." if len(prompt) > 100 else prompt
        strategy_name = self._strategy.__class__.__name__

        for attempt in range(self.config.max_retries):
            try:
                if self.config.timeout_seconds is not None:
                    response = self._call_llm_with_timeout(prompt)
                else:
                    response = self._llm.generate(
                        prompt=prompt,
                        temperature=self.config.temperature,
                    )
                return response

            except FuturesTimeoutError:
                last_error = f"LLM call timed out after {self.config.timeout_seconds}s"
                logger.warning(
                    f"LLM call timeout (attempt {attempt + 1}/{self.config.max_retries}): "
                    f"strategy={strategy_name}, context={context or 'N/A'}, "
                    f"prompt_preview='{prompt_summary}'"
                )

            except Exception as e:
                last_error = str(e)
                logger.warning(
                    f"LLM call failed (attempt {attempt + 1}/{self.config.max_retries}): "
                    f"error={last_error}, strategy={strategy_name}, "
                    f"context={context or 'N/A'}, prompt_preview='{prompt_summary}'"
                )

            if attempt < self.config.max_retries - 1:
                delay = self._calculate_retry_delay(attempt)
                logger.debug(f"Waiting {delay:.2f}s before retry (with jitter)")
                time.sleep(delay)

        raise MetaReasoningError(
            f"Meta-reasoning failed after {self.config.max_retries} attempts "
            f"(strategy={strategy_name}, context={context or 'N/A'})",
            attempts=self.config.max_retries,
            last_error=last_error,
        )

    def _call_llm_with_timeout(self, prompt: str) -> str:
        """Call LLM with timeout protection using ThreadPoolExecutor.

        Args:
            prompt: Prompt to send to LLM

        Returns:
            LLM response string

        Raises:
            FuturesTimeoutError: If timeout is exceeded
        """
        with ThreadPoolExecutor(max_workers=1) as executor:
            future = executor.submit(
                self._llm.generate,
                prompt=prompt,
                temperature=self.config.temperature,
            )
            return future.result(timeout=self.config.timeout_seconds)

    def _parse_json_response(self, response: str) -> Dict[str, Any]:
        """Parse JSON from LLM response.

        Args:
            response: Raw LLM response

        Returns:
            Parsed JSON dictionary

        Raises:
            ValueError: If JSON parsing fails
        """
        # Try to find JSON in the response
        response = response.strip()

        # Remove markdown code blocks if present
        if response.startswith("```json"):
            response = response[7:]
        elif response.startswith("```"):
            response = response[3:]
        if response.endswith("```"):
            response = response[:-3]

        response = response.strip()

        try:
            return json.loads(response)
        except json.JSONDecodeError as e:
            # Try to find JSON object in response
            start = response.find("{")
            end = response.rfind("}") + 1
            if start >= 0 and end > start:
                try:
                    return json.loads(response[start:end])
                except json.JSONDecodeError:
                    pass
            raise ValueError(f"Failed to parse JSON response: {e}")

    def _generate_insight_id(self) -> str:
        """Generate a unique insight ID."""
        return f"INS-{datetime.now().strftime('%Y%m%d%H%M%S')}-{random.randint(1000, 9999)}"

    def _generate_result_id(self) -> str:
        """Generate a unique result ID."""
        return f"MR-{datetime.now().strftime('%Y%m%d%H%M%S')}-{random.randint(1000, 9999)}"

    def analyze_failure_patterns(self, failures: List[FailureRecord]) -> MetaReasoningResult:
        """Identify patterns in reasoning failures.

        Analyzes a list of failure records to identify recurring patterns,
        root causes, and generate actionable insights.

        Args:
            failures: List of failure records to analyze

        Returns:
            MetaReasoningResult with identified patterns and insights

        Raises:
            ValueError: If input validation fails
            MetaReasoningError: If analysis fails after retries
        """
        start_time = time.time()
        self._total_analyses += 1

        if not failures:
            logger.info("No failures to analyze")
            return MetaReasoningResult(
                result_id=self._generate_result_id(),
                analysis_type="failure_analysis",
                insights=[],
                confidence=1.0,
                reasoning_trace=["No failures provided"],
            )

        # Generate cache key
        failure_data = json.dumps(
            [{"id": f.failure_id, "cat": f.category.value} for f in failures],
            sort_keys=True,
        )
        cache_key = self._generate_cache_key("failure_analysis", failure_data)

        # Check cache
        cached = self._get_cached_result(cache_key)
        if cached:
            return cached

        # Prepare prompt
        prompt = self._strategy.prepare_failure_analysis_prompt(failures)
        self._validate_input(prompt, "failure_analysis_prompt")

        try:
            # Call LLM
            response = self._call_llm_with_retry(prompt)
            parsed = self._parse_json_response(response)

            # Extract insights
            insights = []

            # Process patterns
            for pattern in parsed.get("patterns", []):
                insights.append(
                    Insight(
                        insight_id=self._generate_insight_id(),
                        insight_type=InsightType.PATTERN_IDENTIFIED,
                        title=f"Pattern: {pattern.get('pattern_id', 'Unknown')}",
                        description=pattern.get("description", ""),
                        evidence=pattern.get("examples", []),
                        confidence=pattern.get("frequency", 0.5),
                        priority=pattern.get("severity", "medium"),
                    )
                )

            # Process root causes
            for cause in parsed.get("root_causes", []):
                insights.append(
                    Insight(
                        insight_id=self._generate_insight_id(),
                        insight_type=InsightType.ROOT_CAUSE,
                        title=f"Root Cause: {cause.get('cause_id', 'Unknown')}",
                        description=cause.get("description", ""),
                        confidence=cause.get("confidence", 0.5),
                        related_failures=cause.get("affected_patterns", []),
                    )
                )

            # Process recommendations
            for rec in parsed.get("recommendations", []):
                insights.append(
                    Insight(
                        insight_id=self._generate_insight_id(),
                        insight_type=InsightType.STRATEGY_RECOMMENDATION,
                        title=rec.get("title", "Recommendation"),
                        description=rec.get("description", ""),
                        recommended_action=rec.get("expected_impact", ""),
                        priority=rec.get("priority", "medium"),
                        actionable=True,
                    )
                )

            self._total_insights_generated += len(insights)

            result = MetaReasoningResult(
                result_id=self._generate_result_id(),
                analysis_type="failure_analysis",
                insights=insights,
                confidence=parsed.get("confidence", 0.5),
                reasoning_trace=[parsed.get("reasoning_summary", "")],
                processing_time_ms=(time.time() - start_time) * 1000,
            )

            self._successful_analyses += 1
            self._cache_result(cache_key, result)

            logger.info(
                f"Failure analysis complete: {len(insights)} insights, "
                f"confidence={result.confidence:.2f}"
            )

            return result

        except Exception as e:
            self._failed_analyses += 1
            logger.error(f"Failure analysis failed: {e}")
            raise

    def suggest_strategy_changes(self, performance_data: Dict[str, Any]) -> List[StrategyChange]:
        """Recommend reasoning strategy modifications.

        Analyzes performance data across strategies and contexts to
        recommend optimal strategy selections.

        Args:
            performance_data: Performance metrics by strategy

        Returns:
            List of recommended strategy changes

        Raises:
            MetaReasoningError: If analysis fails after retries
        """
        self._total_analyses += 1

        if not performance_data:
            logger.info("No performance data to analyze")
            return []

        # Prepare prompt
        prompt = self._strategy.prepare_strategy_evaluation_prompt(performance_data)

        try:
            response = self._call_llm_with_retry(prompt)
            parsed = self._parse_json_response(response)

            changes = []

            for rec in parsed.get("recommendations", []):
                changes.append(
                    StrategyChange(
                        change_id=f"SC-{datetime.now().strftime('%Y%m%d%H%M%S')}-"
                        f"{random.randint(1000, 9999)}",
                        current_strategy=rec.get("current_strategy", ""),
                        recommended_strategy=rec.get("recommended_strategy", ""),
                        context_conditions=rec.get("conditions", {}),
                        expected_improvement=rec.get("expected_improvement", ""),
                        confidence=rec.get("confidence", 0.5),
                        rationale=rec.get("rationale", ""),
                    )
                )

            self._successful_analyses += 1
            logger.info(f"Strategy evaluation complete: {len(changes)} recommendations")

            return changes

        except Exception as e:
            self._failed_analyses += 1
            logger.error(f"Strategy evaluation failed: {e}")
            raise

    def optimize_prompts(self, prompt: str, failures: List[FailureRecord]) -> List[str]:
        """Suggest prompt improvements based on failure analysis.

        Analyzes a prompt in the context of associated failures to
        suggest specific improvements.

        Args:
            prompt: Current prompt to optimize
            failures: Related failures

        Returns:
            List of prompt improvement suggestions

        Raises:
            ValueError: If input validation fails
            MetaReasoningError: If analysis fails after retries
        """
        self._total_analyses += 1

        if not prompt:
            logger.info("No prompt to optimize")
            return []

        # Validate and sanitize input
        self._validate_input(prompt, "prompt")
        prompt = self._sanitize_input(prompt)

        # Prepare prompt
        analysis_prompt = self._strategy.prepare_prompt_optimization_prompt(prompt, failures)

        try:
            response = self._call_llm_with_retry(analysis_prompt)
            parsed = self._parse_json_response(response)

            improvements = []

            # Extract improvements
            for imp in parsed.get("improvements", []):
                improvement_text = imp.get("description", "")
                if imp.get("after"):
                    improvement_text += f"\n\nSuggested change: {imp.get('after')}"
                improvements.append(improvement_text)

            # Add issues as improvement suggestions
            for issue in parsed.get("issues_identified", []):
                if issue.get("severity") in ("high", "medium"):
                    improvements.append(
                        f"Fix {issue.get('severity')} issue: {issue.get('description')}"
                    )

            self._successful_analyses += 1
            logger.info(f"Prompt optimization complete: {len(improvements)} suggestions")

            return improvements

        except Exception as e:
            self._failed_analyses += 1
            logger.error(f"Prompt optimization failed: {e}")
            raise

    def assess_self_improvement(self, metrics: Dict[str, Any]) -> ImprovementReport:
        """Evaluate system's self-improvement trajectory.

        Performs comprehensive self-assessment based on system metrics,
        generating an improvement report with trajectory analysis,
        insights, and recommended next goals.

        Args:
            metrics: System metrics

        Returns:
            ImprovementReport with assessment

        Raises:
            MetaReasoningError: If analysis fails after retries
        """
        self._total_analyses += 1

        if not metrics:
            logger.info("No metrics to assess")
            return ImprovementReport(
                report_id=f"IR-{datetime.now().strftime('%Y%m%d%H%M%S')}",
                overall_trajectory="unknown",
                next_goals=["Collect more metrics for assessment"],
            )

        # Prepare prompt
        prompt = self._strategy.prepare_self_assessment_prompt(metrics)

        try:
            response = self._call_llm_with_retry(prompt)
            parsed = self._parse_json_response(response)

            # Build insights from parsed data
            insights = []

            for strength in parsed.get("strengths", []):
                insights.append(
                    Insight(
                        insight_id=self._generate_insight_id(),
                        insight_type=InsightType.PATTERN_IDENTIFIED,
                        title=f"Strength: {strength.get('area', 'Unknown')}",
                        description=strength.get("evidence", ""),
                        confidence=strength.get("confidence", 0.7),
                        actionable=False,
                        priority="low",
                    )
                )

            for gap in parsed.get("gaps", []):
                insights.append(
                    Insight(
                        insight_id=self._generate_insight_id(),
                        insight_type=InsightType.KNOWLEDGE_GAP,
                        title=f"Gap: {gap.get('area', 'Unknown')}",
                        description=gap.get("evidence", ""),
                        priority=gap.get("severity", "medium"),
                        actionable=True,
                    )
                )

            # Extract next goals
            next_goals = []
            for goal in parsed.get("next_goals", []):
                if isinstance(goal, dict):
                    next_goals.append(goal.get("goal", str(goal)))
                else:
                    next_goals.append(str(goal))

            # Determine trajectory
            trajectory = parsed.get("overall_trajectory", "stable")
            if isinstance(parsed.get("calibrated_assessment"), dict):
                trajectory = parsed["calibrated_assessment"].get("trajectory", trajectory)
            if isinstance(parsed.get("trend_analysis"), dict):
                trajectory = parsed["trend_analysis"].get("direction", trajectory)

            report = ImprovementReport(
                report_id=f"IR-{datetime.now().strftime('%Y%m%d%H%M%S')}-"
                f"{random.randint(1000, 9999)}",
                overall_trajectory=trajectory,
                metrics_summary=metrics,
                insights=insights,
                confidence_calibration=parsed.get("confidence_calibration", {}),
                next_goals=next_goals,
            )

            self._successful_analyses += 1
            self._total_insights_generated += len(insights)

            logger.info(
                f"Self-assessment complete: trajectory={trajectory}, "
                f"{len(insights)} insights, {len(next_goals)} goals"
            )

            return report

        except Exception as e:
            self._failed_analyses += 1
            logger.error(f"Self-assessment failed: {e}")
            raise

    def set_strategy(self, strategy_type: MetaReasoningStrategyType) -> None:
        """Change the meta-reasoning strategy.

        Args:
            strategy_type: New strategy type to use
        """
        self._strategy = create_meta_reasoning_strategy(strategy_type)
        logger.info(f"Changed strategy to {strategy_type.value}")

    def get_statistics(self) -> Dict[str, Any]:
        """Get meta-reasoner statistics.

        Returns:
            Dictionary containing statistics
        """
        return {
            "total_analyses": self._total_analyses,
            "successful_analyses": self._successful_analyses,
            "failed_analyses": self._failed_analyses,
            "success_rate": (
                self._successful_analyses / self._total_analyses
                if self._total_analyses > 0
                else 0.0
            ),
            "total_insights_generated": self._total_insights_generated,
            "cache_hits": self._cache_hits,
            "cache_size": len(self._cache),
            "current_strategy": self._strategy.strategy_type.value,
        }

    def get_cache_statistics(self) -> Dict[str, Any]:
        """Get detailed cache statistics.

        Returns:
            Dictionary containing cache metrics
        """
        with self._cache_lock:
            cache_size = len(self._cache)

        return {
            "cache_size": cache_size,
            "cache_max_size": self.config.cache_max_size,
            "cache_hits": self._cache_hits,
            "cache_hit_rate": (
                self._cache_hits / self._total_analyses if self._total_analyses > 0 else 0.0
            ),
            "cache_utilization": (
                cache_size / self.config.cache_max_size if self.config.cache_max_size > 0 else 0.0
            ),
            "cache_enabled": self.config.enable_cache,
        }

    def reset_statistics(self) -> None:
        """Reset all statistics counters."""
        self._total_analyses = 0
        self._successful_analyses = 0
        self._failed_analyses = 0
        self._total_insights_generated = 0
        self._cache_hits = 0
        logger.debug("Reset meta-reasoner statistics")

    def clear_cache(self) -> None:
        """Clear the result cache."""
        with self._cache_lock:
            self._cache.clear()
        logger.debug("Cleared meta-reasoner cache")
